{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PATH HERE\n",
    "path_home = \"/PATH/TO/ghlda\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = \"train\"\n",
    "data = \"wiki\"\n",
    "method = \"lda\"\n",
    "word_embedding = \"noembedding\" # glove,word2vec,fasttext\n",
    "date = \"20200105\"\n",
    "file_corpus = data + \"_train_5000.txt\"\n",
    "file_corpus_test = data + \"_test_1000.txt\"\n",
    "\n",
    "path_file =  path_home + \"data/\" + data + \"/\"\n",
    "\n",
    "# Specify word embedding type and path\n",
    "path_emb = path_home + \"data/\" \n",
    "\n",
    "# Specify cpp path\n",
    "path_cpp = path_home + \"cpp\"\n",
    "\n",
    "# Specify save path\n",
    "path_save = path_home + \"results/\" + data + \"/\"\n",
    "\n",
    "# Sepcify file header\n",
    "file_header = file_corpus.split(\".\")[0] + \"_\" + word_embedding + \"_\" + method + \"_\" + date\n",
    "print(\"file name starts with:\" + file_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECIDE TOPIC STRUCTURE HERE\n",
    "num_topics = 40\n",
    "\n",
    "# Leave this alone\n",
    "word_thres = 49\n",
    "kappa = 0.1\n",
    "alpha_init = 0.1\n",
    "nu_add = 0.1\n",
    "beta = alpha_init\n",
    "\n",
    "# remove existing result\n",
    "if train_or_test == \"train\":\n",
    "    current = os.getcwd()\n",
    "    os.chdir(path_save)\n",
    "    print(path_save)\n",
    "    all_files = os.listdir()\n",
    "    hantei = 0\n",
    "    try:\n",
    "        for i in range(len(all_files)):\n",
    "            file_t = all_files[i]\n",
    "            if (file_t != \"old\") & (file_t != \"FIN\"):\n",
    "                #print(file_t)\n",
    "                v1 = file_t.split(\"_\")\n",
    "                if (v1[0] == data) & (v1[2] == version) & (v1[4] == word_embedding) & (v1[5] == method) & (v1[6] == date):\n",
    "                    os.remove(file_t)\n",
    "                    print(\"removed: \" + file_t)\n",
    "                    hantei = 1\n",
    "    except:\n",
    "        pass\n",
    "    if hantei == 0:\n",
    "        print(\"removed none\")\n",
    "    os.chdir(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "type2word2vec = {}\n",
    "if train_or_test == \"train\":\n",
    "    f = open(path_emb + \"glove.6B.50d.txt\", \"r\")\n",
    "    vectors = f.readlines()\n",
    "    word2vec = {}\n",
    "    for vec in vectors:\n",
    "        vec_list = vec.strip().split(\" \")\n",
    "        temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "        word2vec.update({vec_list[0]:temp_vec}) \n",
    "    type2word2vec.update({\"glove\":word2vec})\n",
    "    print(\"Finished Loading glove\")\n",
    "    word2vec = {}\n",
    "    import gensim\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec = KeyedVectors.load_word2vec_format(path_emb  + 'GoogleNews-vectors-negative300.bin', \n",
    "                                              binary=True)\n",
    "    type2word2vec.update({\"word2vec\":word2vec})\n",
    "    print(\"Finished Loading word2vec\")\n",
    "\n",
    "    f = open(path_emb + \"wiki-news-300d-1M.vec\", \"r\")\n",
    "    vectors = f.readlines()\n",
    "    word2vec = {}\n",
    "    for i in range(1,len(vectors)):\n",
    "        vec_list = vectors[i].strip().split(\" \")\n",
    "        temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "        word2vec.update({vec_list[0]:temp_vec})   \n",
    "    type2word2vec.update({\"fasttext\":word2vec})\n",
    "    print(\"Finished Loading fasttext\")\n",
    "    print(\"We are using \" + word_embedding)\n",
    "    word2vec = type2word2vec[\"glove\"]\n",
    "    embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "    \n",
    "else:\n",
    "    if word_embedding == \"fasttext\":\n",
    "        f = open(path_emb  + \"wiki-news-300d-1M.vec\", \"r\")\n",
    "        vectors = f.readlines()\n",
    "        word2vec = {}\n",
    "        for i in range(1,len(vectors)):\n",
    "            vec_list = vectors[i].strip().split(\" \")\n",
    "            temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "            word2vec.update({vec_list[0]:temp_vec})   \n",
    "        type2word2vec.update({\"fasttext\":word2vec})\n",
    "        print(\"Finished Loading fasttext\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]    \n",
    "    \n",
    "    elif word_embedding == \"word2vec\":\n",
    "        word2vec = {}\n",
    "        import gensim\n",
    "        from gensim.models import KeyedVectors\n",
    "        word2vec = KeyedVectors.load_word2vec_format(path_emb  + 'GoogleNews-vectors-negative300.bin', \n",
    "                                                  binary=True)\n",
    "        type2word2vec.update({\"word2vec\":word2vec})\n",
    "        print(\"Finished Loading word2vec\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "        \n",
    "    else:\n",
    "        f = open(path_emb + \"glove.6B.50d.txt\", \"r\")\n",
    "        vectors = f.readlines()\n",
    "        word2vec = {}\n",
    "        for vec in vectors:\n",
    "            vec_list = vec.strip().split(\" \")\n",
    "            temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "            word2vec.update({vec_list[0]:temp_vec}) \n",
    "        type2word2vec.update({\"glove\":word2vec})\n",
    "        print(\"Finished Loading glove\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "        \n",
    "f = open(path_file + file_corpus, \"r\")\n",
    "corpus_data = f.readlines()\n",
    "\n",
    "\n",
    "file_temp = path_emb + \"/\" + data + \"/\" + data + \"_word2cnt.csv\"\n",
    "df_word2cnt = pd.read_csv(file_temp)\n",
    "word2cnt = {}\n",
    "for i in range(len(df_word2cnt)):\n",
    "    word2cnt.update({df_word2cnt[\"word\"].iloc[i]:df_word2cnt[\"cnt\"].iloc[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_word_0 = pd.DataFrame(pd.Series(word2cnt))\n",
    "df_word_0.reset_index(inplace=True)\n",
    "df_word_0.columns=[\"word\",\"cnt\"]\n",
    "cond = df_word_0[\"cnt\"] > word_thres\n",
    "df_word = df_word_0.loc[cond]\n",
    "print(np.sum(cond))\n",
    "print(np.sum(~cond))\n",
    "\n",
    "word2cnt_thres = {}\n",
    "for i in range(len(df_word)):\n",
    "    word2cnt_thres.update({df_word[\"word\"].iloc[i]:df_word[\"cnt\"].iloc[i]})\n",
    "    \n",
    "doc_word_topic_list = []\n",
    "id2word, word2id, doc2cnt = ({} for _ in range(3))\n",
    "id_cnt = 0\n",
    "embedding_agg = 0 * word2vec[\"the\"]\n",
    "for index, doc in enumerate(corpus_data):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:\n",
    "        if word in word2cnt_thres:\n",
    "            if word in word2id:\n",
    "                word_id = word2id[word]\n",
    "            else:\n",
    "                word2id.update({word: id_cnt})\n",
    "                id2word.update({id_cnt: word})\n",
    "                embedding_agg += word2vec[word]\n",
    "                word_id = id_cnt\n",
    "                id_cnt += 1\n",
    "            if int(index) in doc2cnt:\n",
    "                doc2cnt[int(index)] += 1\n",
    "            else:\n",
    "                doc2cnt.update({int(index):1})\n",
    "                \n",
    "            temp_vec = word2vec[word]\n",
    "            doc_word_topic_list.append([int(index),int(word_id),\n",
    "                                        random.randint(0,num_topics-1),\n",
    "                                        0,0,0,0,0])\n",
    "doc_word_topic = np.asarray(doc_word_topic_list)\n",
    "num_docs = max(doc_word_topic[:,0]) + 1        \n",
    "   \n",
    "word2cdf = {}\n",
    "df_word_2 = df_word.sort_values(by=\"cnt\",ascending=False)\n",
    "df_word_2.reset_index(inplace=True)\n",
    "df_word_2.columns=[\"ignore\",\"word\",\"cnt\"]\n",
    "agg = np.sum(df_word_2[\"cnt\"])\n",
    "temp_agg = 0.0\n",
    "for i in range(len(df_word_2)):\n",
    "    temp_agg += float(df_word_2.loc[df_word_2.index==i,\"cnt\"])\n",
    "    df_word_2.loc[df_word_2.index==i,\"cnt\"]  = temp_agg / agg\n",
    "    word = int(word2id[df_word_2[\"word\"].iloc[i]])\n",
    "    cnt = float(df_word_2[\"cnt\"].iloc[i])\n",
    "    word2cdf.update({word:cnt}) \n",
    "\n",
    "embedding_matrix_list = []            \n",
    "embedding_center = embedding_agg / len(word2id)\n",
    "embedding_center = np.reshape(embedding_center,[1,-1])\n",
    "embedding_covariance = np.zeros(shape=(embedding_dimension,embedding_dimension))\n",
    "for i in range(len(word2id)):\n",
    "    word = id2word[i]\n",
    "    temp_vec = word2vec[word] \n",
    "    embedding_matrix_list.append(temp_vec)\n",
    "    embedding_covariance += (temp_vec - embedding_center).transpose() *  (temp_vec - embedding_center)\n",
    "embedding_matrix = np.asarray(embedding_matrix_list)\n",
    "normalized_covariance = embedding_covariance / len(word2id)\n",
    "\n",
    "df_doc = pd.DataFrame(pd.Series(doc2cnt))\n",
    "df_doc.reset_index(inplace=True)\n",
    "df_doc.columns = [\"doc\",\"cnt\"]\n",
    "df_doc.sort_values(by=\"cnt\",ascending=True,inplace=True)\n",
    "print(df_doc.head())\n",
    "print(doc_word_topic.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GHLDA Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghlda\n",
    "GHLDA = ghlda.GHLDA(\"\")\n",
    "\n",
    "# Pass parameters\n",
    "GHLDA.num_topics = num_topics\n",
    "GHLDA.alpha_topic_vec = np.zeros(num_topics) + alpha_init\n",
    "GHLDA.beta = beta\n",
    "GHLDA.voc = len(id2word)\n",
    "\n",
    "# Create initial assignment\n",
    "GHLDA.doc_word_topic = doc_word_topic\n",
    "GHLDA.create_doc2place_vec()\n",
    "GHLDA.lda_calc_tables_parameters_from_assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "epocs_you_want = 1\n",
    "num_thread = 14\n",
    "parallel_loop = num_thread\n",
    "total_inner = 100\n",
    "total = math.floor(epocs_you_want * GHLDA.doc_word_topic.shape[0] / (parallel_loop * total_inner))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run (Use only when training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if train_or_test == \"train\":\n",
    "    saved_epoc_int = {}\n",
    "    for itr in range(total):\n",
    "        num_sampled = (itr+1) * parallel_loop*total_inner\n",
    "        epoc = int(num_sampled / GHLDA.doc_word_topic.shape[0])\n",
    "\n",
    "        # sample\n",
    "        GHLDA.lda_collapsed_gibbs_sample_parallel(total_inner,num_thread,num_thread)\n",
    "\n",
    "        # recalculate parameters\n",
    "        if itr % 500 == 0:\n",
    "            GHLDA.lda_calc_tables_parameters_from_assign()\n",
    "\n",
    "        # save\n",
    "        if (epoc > 0.9) & (epoc not in saved_epoc_int):\n",
    "            saved_epoc_int.update({epoc:1})\n",
    "            file_out = path_save + file_header + \"_epoc_\" + str(epoc) \n",
    "            np.savez_compressed(file_out, \n",
    "                                doc_word_topic = GHLDA.doc_word_topic,\n",
    "                                num_topics = num_topics,\n",
    "                                alpha_topic_vec = GHLDA.alpha_topic_vec,\n",
    "                                beta = GHLDA.beta)\n",
    "            print(\"saved:\" + file_out)   \n",
    "            print(\"num_topics:\" + str(len(set(GHLDA.doc_word_topic[:,2]))))\n",
    "            print(GHLDA.topic2num)\n",
    "            df_topic_0 = pd.DataFrame({\"est\":GHLDA.doc_word_topic[:,2],\n",
    "                                       \"word\":GHLDA.doc_word_topic[:,1]})\n",
    "            df_topic = pd.DataFrame(df_topic_0.groupby([\"est\",\"word\"]).size())\n",
    "            df_topic.reset_index(inplace=True)\n",
    "            df_topic.columns = [\"est\",\"word\",\"cnt\"]\n",
    "            df_topic.sort_values(by=\"cnt\",ascending=False,inplace=True)\n",
    "            for i in range(num_topics):\n",
    "                cond = df_topic[\"est\"] == i\n",
    "                df_temp = df_topic.loc[cond]\n",
    "                num_show = 15\n",
    "                if(len(df_temp)>0):\n",
    "                    if len(df_temp) < num_show:\n",
    "                        num_show = len(df_temp)\n",
    "                    temp = \"\"\n",
    "                    for j in range(num_show):\n",
    "                        temp += id2word[df_temp[\"word\"].iloc[j]] + \",\"\n",
    "                    print(\"topic \" + str(i) + \":\" + temp)\n",
    "            print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Test Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_file + file_corpus_test, \"r\")\n",
    "corpus_data = f.readlines()\n",
    "word2cnt_test = {}\n",
    "for index, doc in enumerate(corpus_data):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:    \n",
    "        if word in word2cnt:\n",
    "            if word in word2cnt_test:\n",
    "                word2cnt_test[word] += 1\n",
    "            else:\n",
    "                word2cnt_test[word] = 1        \n",
    "df_word_0 = pd.DataFrame(pd.Series(word2cnt_test))\n",
    "df_word_0.reset_index(inplace=True)\n",
    "df_word_0.columns=[\"word\",\"cnt\"]\n",
    "cond = df_word_0[\"cnt\"] > word_thres / 5\n",
    "df_word_test = df_word_0.loc[cond]\n",
    "print(np.sum(cond))\n",
    "print(np.sum(~cond))\n",
    "\n",
    "word2cnt_thres_test = {}\n",
    "for i in range(len(df_word_test)):\n",
    "    word2cnt_thres_test.update({df_word_test[\"word\"].iloc[i]:df_word_test[\"cnt\"].iloc[i]})\n",
    "    \n",
    "doc_word_topic_test_list = []\n",
    "doc2cnt_test = ({} for _ in range(1))\n",
    "embedding_agg = 0 * word2vec[\"the\"]\n",
    "id_cnt = len(word2id)\n",
    "for index, doc in enumerate(corpus_data):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:\n",
    "        if word in word2cnt_thres or word in word2cnt_thres_test:\n",
    "            if word in word2id:\n",
    "                word_id = word2id[word]\n",
    "            else:\n",
    "                word2id.update({word: id_cnt})\n",
    "                id2word.update({id_cnt: word})\n",
    "                embedding_agg += word2vec[word]\n",
    "                word_id = id_cnt\n",
    "                id_cnt += 1\n",
    "            if int(index) in doc2cnt:\n",
    "                doc2cnt[int(index)] += 1\n",
    "            else:\n",
    "                doc2cnt.update({int(index):1})\n",
    "                \n",
    "            temp_vec = word2vec[word]\n",
    "            doc_word_topic_test_list.append([int(index),int(word_id),random.randint(0,num_topics-1),\n",
    "                                        0,0,0,0,0])\n",
    "            \n",
    "doc_word_topic_test = np.asarray(doc_word_topic_test_list)\n",
    "num_docs = max(doc_word_topic_test[:,0]) + 1 \n",
    "\n",
    "word2cdf_test = {}\n",
    "df_word_2 = df_word_test.sort_values(by=\"cnt\",ascending=False)\n",
    "df_word_2.reset_index(inplace=True)\n",
    "df_word_2.columns=[\"ignore\",\"word\",\"cnt\"]\n",
    "agg = np.sum(df_word_2[\"cnt\"])\n",
    "temp_agg = 0.0\n",
    "for i in range(len(df_word_2)):\n",
    "    temp_agg += float(df_word_2.loc[df_word_2.index==i,\"cnt\"])\n",
    "    df_word_2.loc[df_word_2.index==i,\"cnt\"]  = temp_agg / agg\n",
    "    word = int(word2id[df_word_2[\"word\"].iloc[i]])\n",
    "    cnt = float(df_word_2[\"cnt\"].iloc[i])\n",
    "    word2cdf_test.update({word:cnt})             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heldout Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_ids = list(set(doc_word_topic_test[:,0]))\n",
    "GHLDA.doc_word_topic_test = doc_word_topic_test\n",
    "GHLDA.create_doc2place_vec_test()\n",
    "GHLDA.id2word = id2word\n",
    "GHLDA.beta = 0.1\n",
    "GHLDA.voc = len(id2word)\n",
    "\n",
    "num_R = 10\n",
    "burn_in = 0\n",
    "eta_thres = 0.1\n",
    "approx = 1\n",
    "num_threads = 24\n",
    "GHLDA.lda_evaluate_held_out_log_likelihood(\n",
    "    doc_test_ids,num_R,burn_in,approx,num_threads,0)\n",
    "\n",
    "heldout_prob = []\n",
    "for i in range(len(doc_test_ids)):\n",
    "    heldout_prob.append(np.sum(np.log(GHLDA.doc_test2heldout_prob[doc_test_ids[i]])))\n",
    "    \n",
    "import random\n",
    "# configure bootstrap\n",
    "n_iterations = 10000\n",
    "n_size = int(len(heldout_prob) * 1.0)\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = random.choices(heldout_prob, k= n_size)\n",
    "    stats.append(np.mean(train))\n",
    "    \n",
    "lower = np.quantile(stats,0.025)\n",
    "upper = np.quantile(stats,0.975)\n",
    "\n",
    "print(\"num_topics:\" +str(num_topics) + \",held-out log-likelihood:\" + str(np.mean(heldout_prob)) + \"[\" + \\\n",
    "     str(lower) + \",\" + str(upper) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
