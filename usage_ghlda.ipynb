{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.loads\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PATH HERE\n",
    "path_home = \"/PATH/TO/ghlda\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = \"train\"\n",
    "data = \"wiki\"\n",
    "method = \"ghlda-ncrp\"\n",
    "word_embedding = \"glove\" # glove,word2vec,fasttext\n",
    "date = \"20200105\"\n",
    "file_corpus = data + \"_train_5000.txt\"\n",
    "file_corpus_test = data + \"_test_1000.txt\"\n",
    "\n",
    "path_file =  path_home + \"/data/\" + data + \"/\"\n",
    "\n",
    "# Specify word embedding type and path\n",
    "path_emb = path_home + \"/data/\" \n",
    "\n",
    "# Specify cpp path\n",
    "path_cpp = path_home + \"/cpp\"\n",
    "\n",
    "# Specify save path\n",
    "path_save = path_home + \"/results/\" + data + \"/\"\n",
    "\n",
    "# Sepcify file header\n",
    "file_header = file_corpus.split(\".\")[0] + \"_\" + word_embedding + \"_\" + method + \"_\" + date\n",
    "print(\"file name starts with:\" + file_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "branch_list = [1,1,4,4]\n",
    "mult_num_list = [50,40,30,20]\n",
    "max_depth_allowed = 4\n",
    "hyper_pi = 100.0\n",
    "hyper_m = 0.5\n",
    "initial_assignment = 0\n",
    "mix_ratio = 0.5\n",
    "level_gamma = 1.0\n",
    "path_gamma = 0.1\n",
    "nu_add = 0.1\n",
    "kappa = 0.1\n",
    "Psi_base = \"raw\"\n",
    "\n",
    "# Leave this alone\n",
    "word_thres = 49 # above this value\n",
    "level_allocation_type = 1\n",
    "alpha_init = 0.1\n",
    "hyper_pi_run = hyper_pi\n",
    "hyper_m_run = hyper_m \n",
    "mult_eta_list = [2, 1, 0.5, 0.25]\n",
    "mult_string = \"\"\n",
    "branch_string = \"\"\n",
    "for i in range(len(mult_num_list)):\n",
    "    mult_string += str(mult_num_list[i]) + \"-\"\n",
    "mult_string = mult_string.strip(\"-\")\n",
    "for i in range(len(branch_list)):\n",
    "    branch_string += str(branch_list[i]) + \"-\"\n",
    "branch_string = branch_string.strip(\"-\")\n",
    "\n",
    "# remove existing result\n",
    "if train_or_test == \"train\":\n",
    "    current = os.getcwd()\n",
    "    os.chdir(path_save)\n",
    "    print(path_save)\n",
    "    all_files = os.listdir()\n",
    "    hantei = 0\n",
    "    try:\n",
    "        for i in range(len(all_files)):\n",
    "            file_t = all_files[i]\n",
    "            if (file_t != \"old\") & (file_t != \"FIN\"):\n",
    "                #print(file_t)\n",
    "                v1 = file_t.split(\"_\")\n",
    "                if (v1[0] == data) & (v1[2] == version) & (v1[4] == word_embedding) & (v1[5] == method) & (v1[6] == date):\n",
    "                    os.remove(file_t)\n",
    "                    print(\"removed: \" + file_t)\n",
    "                    hantei = 1\n",
    "    except:\n",
    "        pass\n",
    "    if hantei == 0:\n",
    "        print(\"removed none\")\n",
    "    os.chdir(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "type2word2vec = {}\n",
    "if train_or_test == \"train\":\n",
    "    f = open(path_emb + \"glove.6B.50d.txt\", \"r\")\n",
    "    vectors = f.readlines()\n",
    "    word2vec = {}\n",
    "    for vec in vectors:\n",
    "        vec_list = vec.strip().split(\" \")\n",
    "        temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "        word2vec.update({vec_list[0]:temp_vec}) \n",
    "    type2word2vec.update({\"glove\":word2vec})\n",
    "    print(\"Finished Loading glove\")\n",
    "    word2vec = {}\n",
    "    import gensim\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec = KeyedVectors.load_word2vec_format(path_emb  + 'GoogleNews-vectors-negative300.bin', \n",
    "                                              binary=True)\n",
    "    type2word2vec.update({\"word2vec\":word2vec})\n",
    "    print(\"Finished Loading word2vec\")\n",
    "\n",
    "    f = open(path_emb + \"wiki-news-300d-1M.vec\", \"r\")\n",
    "    vectors = f.readlines()\n",
    "    word2vec = {}\n",
    "    for i in range(1,len(vectors)):\n",
    "        vec_list = vectors[i].strip().split(\" \")\n",
    "        temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "        word2vec.update({vec_list[0]:temp_vec})   \n",
    "    type2word2vec.update({\"fasttext\":word2vec})\n",
    "    print(\"Finished Loading fasttext\")\n",
    "    print(\"We are using \" + word_embedding)\n",
    "    word2vec = type2word2vec[word_embedding]\n",
    "    embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "    \n",
    "else:\n",
    "    if word_embedding == \"fasttext\":\n",
    "        f = open(path_emb  + \"wiki-news-300d-1M.vec\", \"r\")\n",
    "        vectors = f.readlines()\n",
    "        word2vec = {}\n",
    "        for i in range(1,len(vectors)):\n",
    "            vec_list = vectors[i].strip().split(\" \")\n",
    "            temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "            word2vec.update({vec_list[0]:temp_vec})   \n",
    "        type2word2vec.update({\"fasttext\":word2vec})\n",
    "        print(\"Finished Loading fasttext\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]    \n",
    "    \n",
    "    elif word_embedding == \"word2vec\":\n",
    "        word2vec = {}\n",
    "        import gensim\n",
    "        from gensim.models import KeyedVectors\n",
    "        word2vec = KeyedVectors.load_word2vec_format(path_emb  + 'GoogleNews-vectors-negative300.bin', \n",
    "                                                  binary=True)\n",
    "        type2word2vec.update({\"word2vec\":word2vec})\n",
    "        print(\"Finished Loading word2vec\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "        \n",
    "    else:\n",
    "        f = open(path_emb + \"glove.6B.50d.txt\", \"r\")\n",
    "        vectors = f.readlines()\n",
    "        word2vec = {}\n",
    "        for vec in vectors:\n",
    "            vec_list = vec.strip().split(\" \")\n",
    "            temp_vec = np.asarray(vec_list[1:]).astype(\"float32\")\n",
    "            word2vec.update({vec_list[0]:temp_vec}) \n",
    "        type2word2vec.update({\"glove\":word2vec})\n",
    "        print(\"Finished Loading glove\")\n",
    "        print(\"We are using \" + word_embedding)\n",
    "        embedding_dimension = word2vec[\"the\"].shape[0]\n",
    "        \n",
    "f = open(path_file + file_corpus, \"r\")\n",
    "corpus_data = f.readlines()\n",
    "\n",
    "\n",
    "file_temp = path_emb + \"/\" + data + \"/\" + data + \"_word2cnt.csv\"\n",
    "df_word2cnt = pd.read_csv(file_temp)\n",
    "word2cnt = {}\n",
    "for i in range(len(df_word2cnt)):\n",
    "    word2cnt.update({df_word2cnt[\"word\"].iloc[i]:df_word2cnt[\"cnt\"].iloc[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_0 = pd.DataFrame(pd.Series(word2cnt))\n",
    "df_word_0.reset_index(inplace=True)\n",
    "df_word_0.columns=[\"word\",\"cnt\"]\n",
    "\n",
    "cond = df_word_0[\"cnt\"] > word_thres\n",
    "print(np.sum(cond))\n",
    "print(np.sum(~cond))\n",
    "\n",
    "df_word = df_word_0.loc[cond]\n",
    "\n",
    "word2cnt_thres = {}\n",
    "for i in range(len(df_word)):\n",
    "    word2cnt_thres.update({df_word[\"word\"].iloc[i]:df_word[\"cnt\"].iloc[i]})\n",
    "    \n",
    "doc_word_topic_list = []\n",
    "id2word, word2id, doc2cnt = ({} for _ in range(3))\n",
    "id_cnt = 0\n",
    "embedding_agg = 0 * word2vec[\"the\"]\n",
    "for index, doc in enumerate(corpus_data):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:\n",
    "        if word in word2cnt_thres:\n",
    "        #if word in word2vec:\n",
    "            if word in word2id:\n",
    "                word_id = word2id[word]\n",
    "            else:\n",
    "                word2id.update({word: id_cnt})\n",
    "                id2word.update({id_cnt: word})\n",
    "                embedding_agg += word2vec[word]\n",
    "                word_id = id_cnt\n",
    "                id_cnt += 1\n",
    "            if int(index) in doc2cnt:\n",
    "                doc2cnt[int(index)] += 1\n",
    "            else:\n",
    "                doc2cnt.update({int(index):1})\n",
    "                \n",
    "            temp_vec = word2vec[word]\n",
    "            doc_word_topic_list.append([int(index),int(word_id),\n",
    "                                        0,0,0,0,0,0])\n",
    "doc_word_topic = np.asarray(doc_word_topic_list)\n",
    "num_docs = max(doc_word_topic[:,0]) + 1        \n",
    "   \n",
    "    \n",
    "word2cdf = {}\n",
    "df_word_2 = df_word.sort_values(by=\"cnt\",ascending=False)\n",
    "df_word_2.reset_index(inplace=True)\n",
    "df_word_2.columns=[\"ignore\",\"word\",\"cnt\"]\n",
    "agg = np.sum(df_word_2[\"cnt\"])\n",
    "temp_agg = 0.0\n",
    "for i in range(len(df_word_2)):\n",
    "    temp_agg += float(df_word_2.loc[df_word_2.index==i,\"cnt\"])\n",
    "    df_word_2.loc[df_word_2.index==i,\"cnt\"]  = temp_agg / agg\n",
    "    word = int(word2id[df_word_2[\"word\"].iloc[i]])\n",
    "    cnt = float(df_word_2[\"cnt\"].iloc[i])\n",
    "    word2cdf.update({word:cnt}) \n",
    "\n",
    "    \n",
    "embedding_matrix_list = []            \n",
    "embedding_center = embedding_agg / len(word2id)\n",
    "embedding_center = np.reshape(embedding_center,[1,-1])\n",
    "embedding_covariance = np.zeros(shape=(embedding_dimension,embedding_dimension))\n",
    "for i in range(len(word2id)):\n",
    "    word = id2word[i]\n",
    "    temp_vec = word2vec[word] \n",
    "    embedding_matrix_list.append(temp_vec)\n",
    "    embedding_covariance += (temp_vec - embedding_center).transpose() *  (temp_vec - embedding_center)\n",
    "embedding_matrix = np.asarray(embedding_matrix_list)\n",
    "normalized_covariance = embedding_covariance / len(word2id)\n",
    "\n",
    "df_doc = pd.DataFrame(pd.Series(doc2cnt))\n",
    "df_doc.reset_index(inplace=True)\n",
    "df_doc.columns = [\"doc\",\"cnt\"]\n",
    "df_doc.sort_values(by=\"cnt\",ascending=True,inplace=True)\n",
    "print(df_doc.head())\n",
    "print(doc_word_topic.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GHLDA Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghlda\n",
    "GHLDA = ghlda.GHLDA(\"\")\n",
    "# Pass parameters\n",
    "GHLDA.word2cdf = word2cdf\n",
    "GHLDA.mix_ratio = mix_ratio\n",
    "GHLDA.level_allocation_type = level_allocation_type\n",
    "GHLDA.max_depth_allowed = max_depth_allowed\n",
    "GHLDA.hyper_pi = hyper_pi\n",
    "GHLDA.hyper_m = hyper_m\n",
    "GHLDA.path_gamma = path_gamma\n",
    "GHLDA.level_gamma = level_gamma\n",
    "nu = embedding_matrix.shape[1]  + nu_add\n",
    "GHLDA.nu = nu\n",
    "GHLDA.kappa = kappa \n",
    "GHLDA.embedding_dimension = embedding_dimension\n",
    "zero_mu = np.zeros([1,embedding_dimension])\n",
    "zero_Sigma = np.zeros([embedding_dimension,embedding_dimension])\n",
    "GHLDA.zero_mu = zero_mu\n",
    "GHLDA.zero_Sigma = zero_Sigma\n",
    "GHLDA.alpha_level_vec = np.zeros(1000) + alpha_init\n",
    "# Pass data\n",
    "GHLDA.doc_word_topic = doc_word_topic\n",
    "GHLDA.embedding_matrix = embedding_matrix\n",
    "GHLDA.embedding_center = embedding_center\n",
    "\n",
    "for i in range(len(mult_num_list)):\n",
    "    GHLDA.level2Psi[i] = mult_num_list[i] * np.identity(embedding_center.shape[1])\n",
    "Psi = GHLDA.level2Psi[0]\n",
    "GHLDA.Psi = Psi\n",
    "\n",
    "# Create initial hierarchy\n",
    "GHLDA.create_initial_hierarchy_rev(branch_list)\n",
    "\n",
    "# Create initial assignment\n",
    "GHLDA.create_initial_assignment(initial_assignment)\n",
    "GHLDA.ghlda_calc_tables_parameters_from_assign()\n",
    "\n",
    "# parameter used when running\n",
    "GHLDA.hyper_pi = hyper_pi_run\n",
    "GHLDA.hyper_m = hyper_m_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "epocs_you_want = 1\n",
    "num_thread = 14\n",
    "parallel_loop = num_thread\n",
    "total_inner = 10\n",
    "total = math.floor(epocs_you_want * num_docs / (parallel_loop * total_inner)) + 1\n",
    "GHLDA.min_diagonal_val = 0.000001\n",
    "base = math.floor(math.log10(np.max(GHLDA.topic2Sigma[0])))\n",
    "if base > 0:\n",
    "    if base % 2 != 0:\n",
    "        base -= 1\n",
    "    GHLDA.shrink = 1 / np.power(10,base)\n",
    "else:\n",
    "    GHLDA.shrink = 1.0\n",
    "track = []\n",
    "print(\"total:\" + str(total))\n",
    "print(\"num epocs:\" + str(parallel_loop*total_inner*total/num_docs))\n",
    "print(\"header:\" + file_header)\n",
    "print(GHLDA.level2Psi[0])\n",
    "print(GHLDA.level2Psi[1])\n",
    "print(GHLDA.level2Psi[2])\n",
    "print(GHLDA.level2Psi[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated Level and True Level\n",
    "df_level_0 = pd.DataFrame({\"level\":GHLDA.doc_word_topic[:,3],\n",
    "                           \"word\":GHLDA.doc_word_topic[:,1]})\n",
    "for i in range(len(branch_list)):\n",
    "    cond = df_level_0[\"level\"] == i\n",
    "    print(np.sum(cond))\n",
    "\n",
    "setting = \"word_\" + str(word_thres) + \\\n",
    "      \",branch_\" + branch_string+ \",path_\" + str(path_gamma) + \\\n",
    "      \",level_\" + str(level_gamma) + \",nu_\" + str(nu_add) + \\\n",
    "      \",mult_\" + str(mult_string) + \",pi_\" + str(int(hyper_pi)) + \",m_\" + str(hyper_m) + \\\n",
    "      \",pi-run_\" + str(int(hyper_pi_run)) + \",m-run_\" + str(hyper_m_run) + \",mix_\" + str(mix_ratio)\n",
    "print(setting)\n",
    "\n",
    "\n",
    "df_level = pd.DataFrame(df_level_0.groupby(['word',\"level\"]).size())\n",
    "df_level.reset_index(inplace=True)\n",
    "df_level.columns = [\"word\",\"level\",\"cnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run (Use only when training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if train_or_test == \"train\":    \n",
    "    saved_epoc_int = {}\n",
    "    for i in range(total):\n",
    "        num_sampled = (i+1) * parallel_loop*total_inner\n",
    "        epoc = int(num_sampled / num_docs)\n",
    "        if epoc < 5:\n",
    "            GHLDA.ghlda_collapsed_gibbs_sample_parallel(total_inner,num_thread,num_thread,verbose)\n",
    "        else:\n",
    "            GHLDA.ghlda_ncrp_collapsed_gibbs_sample_parallel(total_inner,num_thread,num_thread,verbose)\n",
    "            \n",
    "        if GHLDA.stop_hantei == 1:\n",
    "            print(\"epoc: \" + str(num_sampled / num_docs) + \"-> Stopped\")\n",
    "            break\n",
    "      \n",
    "        if i % 2 == 0:\n",
    "            # check\n",
    "            print(\"epoc: \" + str(num_sampled / num_docs))\n",
    "            print(\"num_paths:\" + str(len(GHLDA.path_dict)))\n",
    "            print(\"num_topics:\" + str(len(set(GHLDA.doc_word_topic[:,4]))))\n",
    "            total_path = 0\n",
    "            for k in GHLDA.path2num.keys():\n",
    "                total_path += GHLDA.path2num[k]\n",
    "            total_topic = 0\n",
    "            for k in GHLDA.topic2num.keys():\n",
    "                total_topic += GHLDA.topic2num[k]    \n",
    "            total_level = 0\n",
    "            df_level_0 = pd.DataFrame({\"level\":GHLDA.doc_word_topic[:,3],\n",
    "                                       \"word\":GHLDA.doc_word_topic[:,1]})\n",
    "            for j in range(max_depth_allowed):\n",
    "                cond = df_level_0[\"level\"] == j\n",
    "                print(np.sum(cond))\n",
    "                total_level += np.sum(cond)\n",
    "            print(\"total:\" + str(total_path) + \",\" + str(total_topic) + \",\" + \\\n",
    "                     str(total_level))   \n",
    "            print(\"len_path:\" + str(len(GHLDA.path_dict)) + \",\" + str(len(GHLDA.path2num)))\n",
    "             \n",
    "        # recalculate parameters\n",
    "        if i % 50 == 0:  \n",
    "            # recalculate\n",
    "            GHLDA.ghlda_calc_tables_parameters_from_assign()\n",
    "            \n",
    "        # save\n",
    "        if (epoc > 0.9) & (epoc not in saved_epoc_int):\n",
    "            saved_epoc_int.update({epoc:1})\n",
    "            file_out = path_save + file_header + \"_epoc_\" + str(epoc) \n",
    "            np.savez_compressed(file_out, \n",
    "                                doc_word_topic = GHLDA.doc_word_topic,\n",
    "                                path_dict = GHLDA.path_dict,\n",
    "                                path2num = GHLDA.path2num,\n",
    "                                word_thres = word_thres,\n",
    "                                max_depth_allowed = max_depth_allowed,\n",
    "                                branch_list = branch_list,\n",
    "                                mult_num_list = mult_num_list,\n",
    "                                Psi_0 = GHLDA.level2Psi[0], \n",
    "                                Psi_1 = GHLDA.level2Psi[1], \n",
    "                                Psi_2 = GHLDA.level2Psi[2],\n",
    "                                Psi_3 = GHLDA.level2Psi[3],\n",
    "                                hyper_m = GHLDA.hyper_m,\n",
    "                                hyper_pi = GHLDA.hyper_pi,\n",
    "                                path_gamma = GHLDA.path_gamma,\n",
    "                                level_gamma = GHLDA.level_gamma,\n",
    "                                level_allocation_type = GHLDA.level_allocation_type,\n",
    "                                nu = nu,\n",
    "                                kappa = kappa,\n",
    "                                alpha_topic_vec = GHLDA.alpha_topic_vec)\n",
    "            \n",
    "            # verbose\n",
    "            print(\"epoc: \" + str(num_sampled / num_docs))\n",
    "            print(\"num_paths:\" + str(len(GHLDA.path_dict)))\n",
    "            print(\"num_topics:\" + str(len(set(GHLDA.doc_word_topic[:,4]))))\n",
    "            print(\"----path_dict----\")\n",
    "            print(GHLDA.path_dict)\n",
    "            print(\"----path2num----\")\n",
    "            print(GHLDA.path2num)      \n",
    "            # Estimated Level and True Level\n",
    "            df_topic_0 = pd.DataFrame({\"est\":GHLDA.doc_word_topic[:,4],\n",
    "                                       \"word\":GHLDA.doc_word_topic[:,1]})\n",
    "            df_topic = pd.DataFrame(df_topic_0.groupby([\"est\",\"word\"]).size())\n",
    "            df_topic.reset_index(inplace=True)\n",
    "            df_topic.columns = [\"est\",\"word\",\"cnt\"]\n",
    "            df_topic.sort_values(by=\"cnt\",ascending=False,inplace=True)\n",
    "            uni_topic = list(set(df_topic[\"est\"]))\n",
    "            uni_topic.sort()\n",
    "            if len(uni_topic) > 5:\n",
    "                num_show_topic = 5\n",
    "            else:\n",
    "                num_show_topic = len(uni_topic)\n",
    "            for j in range(len(uni_topic)):\n",
    "            #for j in range(num_show_topic):\n",
    "                cond = df_topic[\"est\"] == uni_topic[j]\n",
    "                df_temp = df_topic.loc[cond]\n",
    "                num_show = 15\n",
    "                if(len(df_temp)>0):\n",
    "                    if len(df_temp) < num_show:\n",
    "                        num_show = len(df_temp)\n",
    "                    temp = \"\"\n",
    "                    for k in range(num_show):\n",
    "                        temp += id2word[df_temp[\"word\"].iloc[k]] + \",\"\n",
    "                    print(\"topic \" + str(uni_topic[j]) + \":\" + temp)\n",
    "            print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Test Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_file + file_corpus_test, \"r\")\n",
    "corpus_data_test = f.readlines()\n",
    "\n",
    "word2cnt_test = {}\n",
    "for index, doc in enumerate(corpus_data_test):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:    \n",
    "        if word in word2vec:\n",
    "            if word in word2cnt_test:\n",
    "                word2cnt_test[word] += 1\n",
    "            else:\n",
    "                word2cnt_test[word] = 1\n",
    "                \n",
    "df_word_0 = pd.DataFrame(pd.Series(word2cnt_test))\n",
    "df_word_0.reset_index(inplace=True)\n",
    "df_word_0.columns=[\"word\",\"cnt\"]\n",
    "\n",
    "cond = df_word_0[\"cnt\"] > word_thres / 5\n",
    "print(np.sum(cond))\n",
    "print(np.sum(~cond))\n",
    "\n",
    "df_word = df_word_0.loc[cond]\n",
    "\n",
    "word2cnt_thres_test = {}\n",
    "for i in range(len(df_word)):\n",
    "    word2cnt_thres_test.update({df_word[\"word\"].iloc[i]:df_word[\"cnt\"].iloc[i]})\n",
    "    \n",
    "f = open(path_file + file_corpus_test, \"r\")\n",
    "corpus_data_test = f.readlines()\n",
    "\n",
    "doc_word_topic_test_list = []\n",
    "doc2cnt_test = {}\n",
    "id2cnt = len(id2word)\n",
    "for index, doc in enumerate(corpus_data_test):  \n",
    "    temp = doc.strip().split(' ')\n",
    "    for word in temp:    \n",
    "        if word in word2vec:\n",
    "            if word in word2cnt_thres:\n",
    "                if word in word2id:\n",
    "                    word_id = word2id[word]\n",
    "                else:\n",
    "                    word2id.update({word: id_cnt})\n",
    "                    id2word.update({id_cnt: word})\n",
    "                    embedding_agg += word2vec[word]\n",
    "                    word_id = id_cnt\n",
    "                    id_cnt += 1\n",
    "\n",
    "                if int(index) in doc2cnt_test:\n",
    "                    doc2cnt_test[int(index)] += 1\n",
    "                else:\n",
    "                    doc2cnt_test.update({int(index):1})\n",
    "                temp_vec = word2vec[word]\n",
    "                doc_word_topic_test_list.append([int(index),int(word_id),\n",
    "                                                0,0,0,0,0,0])\n",
    "doc_word_topic_test = np.asarray(doc_word_topic_test_list)\n",
    "num_docs_test = max(doc_word_topic_test[:,0]) + 1    \n",
    "\n",
    "# Update embedding_matrix\n",
    "for i in range(len(word2id)):\n",
    "    word = id2word[i]\n",
    "    temp_vec = word2vec[word]\n",
    "    embedding_matrix_list.append(temp_vec)\n",
    "embedding_matrix_test = np.asarray(embedding_matrix_list)\n",
    "\n",
    "df_doc = pd.DataFrame(pd.Series(doc2cnt_test))\n",
    "df_doc.reset_index(inplace=True)\n",
    "df_doc.columns = [\"doc\",\"cnt\"]\n",
    "df_doc.sort_values(by=\"cnt\",ascending=True,inplace=True)\n",
    "df_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heldout Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc_test_ids = list(set(doc_word_topic_test[:,0]))\n",
    "GHLDA.doc_word_topic_test = doc_word_topic_test\n",
    "GHLDA.create_doc2place_vec_test()\n",
    "GHLDA.embedding_matrix = embedding_matrix\n",
    "GHLDA.id2word = id2word\n",
    "GHLDA.beta = 0.1\n",
    "GHLDA.voc = len(id2word)\n",
    "\n",
    "num_R = 10\n",
    "burn_in = 0\n",
    "eta_thres = 0.00001\n",
    "approx = 1\n",
    "num_threads = 18\n",
    "GHLDA.ghlda_ncrp_evaluate_held_out_log_likelihood(doc_test_ids,num_R,burn_in,0.0001,approx,num_threads,0)\n",
    "\n",
    "heldout_prob = []\n",
    "for i in range(len(doc_test_ids)):\n",
    "    heldout_prob.append(np.sum(np.log(GHLDA.doc_test2heldout_prob[doc_test_ids[i]])))\n",
    "    \n",
    "import random\n",
    "# configure bootstrap\n",
    "n_iterations = 10000\n",
    "n_size = int(len(heldout_prob) * 1.0)\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = random.choices(heldout_prob, k= n_size)\n",
    "    stats.append(np.mean(train))\n",
    "    \n",
    "lower = np.quantile(stats,0.025)\n",
    "upper = np.quantile(stats,0.975)\n",
    "\n",
    "print( \",held-out log-likelihood:\" + str(np.mean(heldout_prob)) + \"[\" + \\\n",
    "     str(lower) + \",\" + str(upper) + \"]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
